# MiniClawd - Lightweight AI Agent Framework

MiniClawd is a powerful, resource-efficient AI agent framework designed to run on devices ranging from Raspberry Pi to cloud servers. It provides a flexible agentic system with multiple interfaces (CLI, Web UI, Telegram Bot) and extensive tool integration.

## üöÄ Core Concept

MiniClawd transforms a Large Language Model into a **functional autonomous agent** that can:
- **Reason**: Analyze user intent and plan actions
- **Act**: Execute tools and commands in the real world
- **Observe**: Process results and iterate until task completion

The framework operates on a continuous **reason-act-observe loop**, making it capable of complex, multi-step tasks rather than just generating text.

## ‚ö° Power Modes

MiniClawd features **two distinct operating modes** optimized for different hardware capabilities:

### üîã LOW POWER Mode
**Designed for**: Raspberry Pi, Edge Devices, Low-End Hardware

**Optimizations**:
- ‚úÖ Minimal memory footprint (20 messages, ~49KB max)
- ‚úÖ Current session only (no persistent history)
- ‚úÖ Optimized for tiny models (llama3.2:1b, qwen:0.5b)
- ‚úÖ Reduced context window
- ‚úÖ Fast response times
- ‚úÖ Runs on devices with < 2GB RAM

**Perfect for**: IoT devices, Raspberry Pi projects, always-on edge agents

### ‚ö° HIGH POWER Mode  
**Designed for**: Desktop, Cloud, Powerful Servers

**Features**:
- ‚úÖ Full conversation history (100+ messages)
- ‚úÖ Extended context window (~195KB max)
- ‚úÖ Support for larger models (llama3:8b, mistral:7b)
- ‚úÖ Enhanced reasoning capabilities
- ‚úÖ Complex multi-step task execution
- ‚úÖ Complete memory persistence

**Perfect for**: Development workstations, cloud deployments, complex automation

## üöÄ Features

### Core Capabilities
- **Dual Power Modes**: LOW POWER (Raspberry Pi) and HIGH POWER (Desktop/Cloud)
- **Multi-Provider LLM Support**: Local Ollama or cloud OpenAI/Anthropic
- **Autonomous Agent Loop**: Reason ‚Üí Act ‚Üí Observe continuously
- **Tool Registry**: 15+ built-in tools with extensible framework
- **Multiple Interfaces**: CLI, Web Dashboard, Telegram Bot
- **Persistent Memory**: Conversation history with automatic pruning
- **Streaming Support**: Real-time LLM response streaming

### üÜï Raspberry Pi Optimizations (v1.1)
- **Memory Management**: Automatic circular buffer with configurable limits
- **Hardware Metrics Caching**: 5-second TTL to reduce system calls
- **Pre-compiled Regex Patterns**: Faster JSON parsing in agent loop
- **Request Timeouts**: Prevent hanging on slow operations (30s default)
- **Streaming Support**: Reduced latency with LLM response streaming
- **CPU Throttle Detection**: Monitor Raspberry Pi thermal throttling
- **Temperature Alerts**: Automatic warnings when CPU exceeds thresholds

### ü§ñ Autonomous Agent Architecture

MiniClawd implements a **modular orchestration layer** that transforms static LLMs into autonomous agents:

1. **Tool Registry**: Extensible system of capabilities (file ops, web browsing, APIs)
2. **Reasoning Engine**: LLM analyzes context and selects appropriate tools
3. **Execution Layer**: Sandboxed tool execution with error handling
4. **Memory System**: Persistent conversation state and context management
5. **Interface Adapters**: CLI, Web UI, Telegram for multi-channel access

**Reason-Act-Observe Loop**:
```
User Request ‚Üí Agent Reasons ‚Üí Selects Tool ‚Üí Executes Action ‚Üí 
Observes Result ‚Üí Reasons Again ‚Üí ... ‚Üí Final Answer
```

This architecture enables:
- ‚úÖ Multi-step task completion without human intervention
- ‚úÖ Real-world interaction (file system, web, email)
- ‚úÖ Self-correction based on tool feedback
- ‚úÖ Complex workflow automation

### üõ†Ô∏è Built-in Tools

#### System & Performance
- `system_info` - Get CPU, memory, temperature, disk usage
- `check_cpu_throttling` - Detect Raspberry Pi throttling
- `get_memory_stats` - View agent memory utilization
- `clear_memory` - Free up memory resources
- `get_performance_metrics` - Track LLM and tool performance

#### File System
- `list_dir` - Browse directories
- `read_file` - Read file contents
- `write_file` - Create/modify files

#### Execution
- `run_command` - Execute shell commands
- `execute_code` - Run scripts (JS, Python, Bash, PowerShell)

#### Web & API
- `search_web` - DuckDuckGo search
- `read_web_page` - Extract page content
- `weather` - Get weather data (Open-Meteo)
- `crypto_price` - Cryptocurrency prices (CoinGecko)
- `random_fact` - Fun facts

#### Email
- `gmail_auth` - Authenticate with Gmail
- `gmail_send` - Send emails
- `gmail_read` - Read inbox

## üì¶ Installation

### Quick Install (Raspberry Pi)
```bash
curl -fsSL https://raw.githubusercontent.com/stepankarlovec/miniclawd/main/install.sh | bash
```

### Manual Installation
```bash
# Clone repository
git clone https://github.com/stepankarlovec/miniclawd.git
cd miniclawd

# Install dependencies
npm install

# Run configuration wizard
npm run setup

# Start the agent
npm start
```

## ‚öôÔ∏è Configuration

The configuration wizard (`npm run setup`) will help you configure:
- **Power Mode**: LOW POWER (Raspberry Pi) or HIGH POWER (Desktop/Cloud)
- **LLM Provider**: Ollama (local) or OpenAI (API)
- **Model Selection**: Optimized defaults for each power mode
- **Web Server**: Dashboard settings
- **Telegram Bot**: Optional integration
- **Gmail**: Optional email integration

### Power Mode Comparison

| Feature | LOW POWER | HIGH POWER |
|---------|-----------|------------|
| **Target Hardware** | Raspberry Pi, Edge | Desktop, Cloud |
| **Memory Limit** | 20 messages (~49KB) | 100 messages (~195KB) |
| **History** | Current session only | Full conversation |
| **Recommended Models (Ollama)** | llama3.2:1b, qwen:0.5b | llama3:8b, mistral:7b |
| **Recommended Models (OpenAI)** | gpt-4o-mini | gpt-4o |
| **Context Window** | Minimal | Extended |
| **Use Case** | IoT, Edge AI, Always-on | Complex tasks, Development |

## üéØ Usage Examples

### CLI Mode
```bash
npm start
```

### Systemd Service (24/7 Operation)
```bash
# Enable auto-start on boot
sudo systemctl enable miniclawd

# Start service
sudo systemctl start miniclawd

# Check status
sudo systemctl status miniclawd

# View logs
sudo journalctl -u miniclawd -f
```

### Web Dashboard
Access at `http://raspberry-pi-ip:3000`
- Real-time system health monitoring
- CPU temperature, memory, disk usage
- Agent configuration management
- Conversation interface

### Telegram Bot
Configure via setup wizard, then chat with your agent on Telegram!

## üîß Performance Tuning

### LOW POWER Mode - Raspberry Pi 3/4
```json
{
  "llm_provider": "ollama",
  "model_name": "llama3.2:1b",
  "power_mode": "LOW_POWER",
  "enable_streaming": true
}
```

### LOW POWER Mode - Raspberry Pi Zero/1/2
```json
{
  "llm_provider": "ollama",
  "model_name": "qwen:0.5b",
  "power_mode": "LOW_POWER"
}
```

### HIGH POWER Mode - Desktop/Server with GPU
```json
{
  "llm_provider": "ollama",
  "model_name": "llama3:8b",
  "power_mode": "HIGH_POWER",
  "enable_streaming": true
}
```

### Using Cloud API (Both Power Modes)
```json
{
  "llm_provider": "openai",
  "model_name": "gpt-4o-mini",
  "power_mode": "LOW_POWER",
  "openai_api_key": "sk-..."
}
```

## üìä Monitoring

Check system performance:
```
You: Check system info
Agent: (uses system_info tool)

You: Check if CPU is throttling
Agent: (uses check_cpu_throttling tool)

You: Show memory usage
Agent: (uses get_memory_stats tool)
```

## üß™ Testing

Run the test suite to verify installation:
```bash
# Run all tests
npm test

# Run interactive demonstration
npm run test:demo
```

The test suite verifies:
- ‚úÖ LOW POWER mode configuration (20 msgs, ~49KB)
- ‚úÖ HIGH POWER mode configuration (100 msgs, ~195KB)
- ‚úÖ Backward compatibility with legacy profiles
- ‚úÖ Memory limits for each mode
- ‚úÖ Agent reasoning loop execution

## üîí Security Notes

- Gmail OAuth credentials stored in `data/gmail_credentials.json`
- Telegram approval system via `approved_telegram_ids` in config
- File system tools operate within working directory
- Command execution available - restrict access appropriately

## ü§ù Contributing

Contributions welcome! Please ensure:
- Code follows existing patterns
- New tools extend `BaseTool` class
- Performance optimizations preserve functionality
- Documentation updated for new features

## üìù License

ISC License - See LICENSE file

## üêõ Troubleshooting

### High CPU Temperature
```bash
# Check throttling status
vcgencmd get_throttled

# Improve cooling or reduce load by using low profile
```

### Out of Memory
The agent now automatically prunes old messages, but you can manually clear:
```
You: Clear memory
Agent: (uses clear_memory tool)
```

### Ollama Connection Issues

**‚ö†Ô∏è IMPORTANT**: If you're getting "Cannot connect to Ollama" or "Not getting any response":

**Quick Check**:
```bash
# 1. Check if Ollama is running
curl http://127.0.0.1:11434

# 2. Check installed models
ollama list

# 3. Run diagnostic test
npm run test:ollama
```

**Quick Fix**:
```bash
# Install Ollama (if not installed)
curl https://ollama.ai/install.sh | sh

# Start Ollama service
ollama serve

# Pull a recommended model
ollama pull llama3.2:1b    # For Raspberry Pi 3/4
ollama pull qwen:0.5b      # For Raspberry Pi Zero/1/2
```

**üìö Full Troubleshooting Guide**: See [Ollama Troubleshooting Guide](docs/OLLAMA_TROUBLESHOOTING.md) for:
- Complete diagnostics checklist
- Common error messages and solutions
- Performance optimization
- Memory and timeout issues
- Model selection guide

**New in v1.2**: Comprehensive Ollama logging and diagnostics
- Verbose logging shows timing metrics, TTFB, and bottlenecks
- Automatic connectivity checks on errors
- Thinking process extraction and display
- See [Ollama Improvements Guide](docs/OLLAMA_IMPROVEMENTS.md) for details

### Ollama Slow Performance

**Symptoms**: Long delays or timeouts

**Quick Fixes**:
1. Check the console logs for `[Ollama]` timing information
2. Use a smaller model (qwen2.5:0.5b instead of larger variants)
3. Increase timeout in config: `"timeout": 90000`
4. Enable streaming: `"enable_streaming": true`
5. Check CPU throttling with `check_cpu_throttling` tool

**Detailed Diagnostics**: See [Ollama Improvements Guide](docs/OLLAMA_IMPROVEMENTS.md)

## üéì Advanced Topics

### Thinking Process Display

MiniClawd now supports displaying the thinking process from models that use `<think>` tags (like Qwen models).

**In Web UI**:
- Thinking appears in collapsible sections with üß† emoji
- Toggle thinking display with `/thoughts` command
- Thoughts are separated from the final answer

**In CLI**:
- Thoughts are logged with `[Thought]` prefix
- Shown in magenta color
- Can be disabled by setting `verbose: false` in provider options

### Custom Tools
Create new tools by extending `BaseTool`:
```javascript
import { BaseTool } from './base.js';

export class MyCustomTool extends BaseTool {
    constructor() {
        super('my_tool', 'Description', { /* schema */ });
    }
    
    async execute(args) {
        // Your logic here
        return "Result";
    }
}
```

### Memory Limits
Customize in `src/core/agent.js`:
```javascript
const memoryOptions = {
    maxMessages: 50,    // Max conversation turns
    maxSize: 50000      // Max bytes (50KB)
};
```

## üìà Changelog

### v1.2.0 (Current - Power Modes & Ollama Improvements)
- ‚úÖ **LOW POWER Mode**: Optimized for Raspberry Pi/Edge devices (minimal memory, concise prompts)
- ‚úÖ **HIGH POWER Mode**: Full-featured agent for desktop/cloud (extended history, detailed reasoning)
- ‚úÖ **Thinking Process Display** - Extract and show model reasoning in `<think>` tags
- ‚úÖ **Comprehensive Ollama Logging** - Detailed timing metrics and diagnostics
- ‚úÖ **Performance Bottleneck Detection** - TTFB tracking, duration logging
- ‚úÖ **Automatic Ollama Health Checks** - Connectivity verification on errors
- ‚úÖ **Extended Timeouts** - Increased to 60s for slower devices
- ‚úÖ Power mode normalization with backward compatibility
- ‚úÖ Enhanced model selection (support for llama3.2:1b, qwen:0.5b, qwen2.5 models, and larger models)
- ‚úÖ Fixed JSON parsing bug for nested objects
- ‚úÖ Fixed tool execution bug (execute method)
- ‚úÖ Comprehensive test suite (power modes, thinking extraction, Ollama)
- ‚úÖ Updated documentation with power mode and Ollama improvement guides

### v1.1.0
- ‚úÖ Memory auto-pruning with circular buffer
- ‚úÖ Hardware metrics caching (5s TTL)
- ‚úÖ Pre-compiled regex patterns
- ‚úÖ Request timeout handling
- ‚úÖ Streaming response support
- ‚úÖ CPU throttle detection tool
- ‚úÖ Performance metrics tracking
- ‚úÖ Temperature alert system
- ‚úÖ Enhanced system monitoring tools

### v1.0.0
- Initial release
- Basic agent framework
- Tool system
- Web/Telegram interfaces

---

Made with ‚ù§Ô∏è for Raspberry Pi and edge AI
